{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Cloudera Hbase NiFi Hands on Lab","text":""},{"location":"index.html#cloudera-hbase-nifi-hands-on-lab","title":"Cloudera Hbase NiFi Hands on Lab","text":"<p>The goal of this workshop is to have fun!!!</p> <p>What to expect</p> <p>This scavenger hunt Hands on Lab challenges you to understand a real-time fraud detection system built on Cloudera's leading data stack.</p> <p>Good time!</p>"},{"location":"index.html#the-stack-today","title":"The Stack Today","text":"<ul> <li>Hbase: Column-oriented NoSQL datastore that provides real-time, random read/write access ontop HDFS</li> <li>Phoenix: Enables low-latency SQL querying and OLTP applications directly on top of HBase</li> <li>HUE: Web interface and SQL assistant that simplifies querying, analyzing, and managing data.</li> <li>MiNiFi: Generates synthetic transaction events and routes them to Kafka.</li> <li>Edge Flow Manager (EFM): Manages the entire fleet of deployed MiNiFi agents.</li> <li>DataFlow/NiFi: Copy transaction events from edgeTopic to fraudulent Topic for further analysis</li> <li>Schema Registry: Stores and manages the schema for transaction events.</li> <li>Kafka: Receives and distributes transaction events to consumers.</li> </ul> <p>Tip</p> <p>Use the workshop instructions to follow the flow, but also use the time to explore the product and learn new things and tricks. We expect you to be more confident with the product and lead workshops with customers with what you learn here today.</p> <ul> <li> <p> welcome. Access Cloudera Environment</p> <p>Goals</p> <ul> <li> Use Case Overview</li> <li> Connect to Cloudera on cloud control plane</li> </ul> <p> Go to Lab 0 Guide</p> </li> <li> <p> Lab 1. hbase all the way</p> <p>Goals</p> <ul> <li> Explore Edge Flow Manager interface</li> <li> Monitor and manage edge MiNiFi agents</li> </ul> <p> Go to Hbase Lab Guide</p> </li> <li> <p> Lab 2. NIFI - Edge to AI</p> <p>Goals</p> <ul> <li> Explore Edge Flow Manager interface</li> <li> Monitor and manage edge MiNiFi agents</li> </ul> <p> Go to NiFi Lab Guide</p> </li> </ul>"},{"location":"labs/hbase/index.html","title":"Cloudera Hbase Hands on Lab","text":""},{"location":"labs/hbase/index.html#cloudera-hbase-hands-on-lab","title":"Cloudera Hbase Hands on Lab","text":"<p>The goal of this workshop is to have fun!!!</p> <p>What to expect</p> <p>You will learn how to optimize a Phoneix Table in order to get the best performance for your applications good time!</p>"},{"location":"labs/hbase/index.html#the-stack","title":"The Stack","text":"<ul> <li>Hbase: Real time wide column Database</li> <li>Phoenix: SQL languange Over Hbase</li> </ul> <p>Tip</p> <p>Use the workshop instructions to follow the flow, but also use the time to explore the product and learn new things and tricks. We expect you to be more confident with the product and lead workshops with customers with what you learn here today.</p> <ul> <li> <p> Lab 0. Access Cloudera Environment</p> <p>Goals     - [ ] Use Case Overview     - [ ] Connect to Cloudera on cloud control plane</p> <p> Go to Lab 0 Guide</p> </li> <li> <p> Lab 1. Loading data to HBase</p> <p>Goals     - [ ] Create the Phoenix connection     - [ ] Creating your first table     - [ ] Loading dummy data into the table</p> <p> Go to Lab 1 Guide</p> </li> <li> <p> Lab 2. Query the tables data</p> <p>Goals     - [ ] Login to HUE     - [ ] Query the Tables data and understand what happens behind the scenes</p> <p> Go to Lab 2 Guide</p> </li> <li> <p> Lab 3. Optimizong HBase tables</p> <p>Goals     - [ ] Create the new HBase table that will get better performance     - [ ] Loading the data from the old table     - [ ] Creating index over the data     - [ ] Understanding range scans</p> <p> Go to Lab 3 Guide</p> </li> <li> <p> Lab 4. Query the optimized tables data</p> <p>Goals     - [ ] Query the Tables data and understand what happens behind the scenes</p> <p> Go to Lab 4 Guide</p> </li> </ul> <p></p>"},{"location":"labs/hbase/lab1/index.html","title":"Lab 1 - Loading data to HBase","text":""},{"location":"labs/hbase/lab1/index.html#loading-data-to-hbase","title":"Loading data to HBase","text":"<p>Goals</p> <ul> <li> Create the Phoenix connection</li> <li> Creating your first table</li> <li> Loading dummy data into the table</li> </ul>"},{"location":"labs/hbase/lab1/index.html#overview","title":"Overview","text":"<p>You need to check if things are running smoothly In order to do it we can connect to hbase shell.</p>"},{"location":"labs/hbase/lab1/index.html#tasks","title":"Tasks","text":"<p>First we would go to your user profile and create a Workload Password and SSH access key</p> <p>Important</p> <p>The tasks below mention user-0XX, where <code>XX</code> matches your assigned Scavenger Hunt UserID. All Python code will run on CAI</p> <ul> <li> <p>Install phoenixdb </p><pre><code>!pip install phoenixdb\n</code></pre><p></p> </li> <li> <p>create the Phoenix connection</p> </li> </ul> <pre><code>import phoenixdb\nimport phoenixdb.cursor\nfrom datetime import datetime\nimport random\nimport time\nimport urllib3\n\n# 1. SETUP &amp; AUTHENTICATION\n# Suppress SSL warnings for lab environments\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Replace with your specific Cloudera Workload Credentials\nopts = {\n    'authentication': 'BASIC',\n    'user': '&lt;User&gt;', \n    'password': '&lt;WL Pasword&gt;',\n    'verify': False \n}\n\n\nurl = \"https://cod-1eevbst1maraf-gateway.cldr-gre.yovj-8g7d.cloudera.site/cod-1eevbst1maraf/cdp-proxy-api/avatica/\"\n\nprint(\"Connecting to Cloudera Phoenix...\")\nconn = phoenixdb.connect(url, autocommit=True, **opts)\ncursor = conn.cursor()\n</code></pre> <ul> <li>Creating your first table</li> <li>This table will be holding a primary key with EVENT_TIME and SENSOR_ID</li> </ul> <pre><code>   print(\"\\n[Step 1] Creating Naive Table...\")\n    # Design Flaw: Leading with Time causes write hotspots and slow sensor searches.\ncursor.execute(\"\"\"CREATE SCHEMA IF NOT EXISTS SENSORS_&lt;userxxx&gt;\"\"\")\n\n\ncursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt; (\n            EVENT_TIME TIMESTAMP NOT NULL,\n            SENSOR_ID VARCHAR NOT NULL,\n            READING DOUBLE,\n            STATUS_MSG VARCHAR,\n            CONSTRAINT pk PRIMARY KEY (EVENT_TIME, SENSOR_ID)\n        )\n    \"\"\")\n</code></pre> <ul> <li>Loading dummy data into the table:</li> </ul> <pre><code>print(\"[Step 2] Loading 50,000 rows into Naive Table (Batching 500)...\")\nbatch_size = 200\nsensors = [f'SENSOR_{i:03}' for i in range(1, 51)]\nerrors = [\"OK\", \"CONNECTION_TIMEOUT\", \"LOW_VOLTAGE\", \"SENSOR_STUCK\"]\n\ndata_to_insert = []\nfor i in range(1, 50001):\n    row = (datetime.now(), random.choice(sensors), random.uniform(20, 90), random.choice(errors))\n    data_to_insert.append(row) \n    if i % batch_size == 0:\n        cursor.executemany(\"UPSERT INTO SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt; VALUES (?, ?, ?, ?)\", data_to_insert)\n        data_to_insert = []\n        if i % 1000 == 0: print(f\"  Progress: {i}/50000...\")\n</code></pre>"},{"location":"labs/hbase/lab1/index.html#checklist","title":"Checklist","text":"<ul> <li> Are all our tables healthy?</li> </ul> <p> We have now concluded Lab 1 </p>"},{"location":"labs/hbase/lab2/index.html","title":"Lab 2 - Query the tables data","text":""},{"location":"labs/hbase/lab2/index.html#query-the-tables-data","title":"Query the tables data","text":"<p>Goals</p> <ul> <li> Login to HUE </li> <li> Query the Tables data and understand what happens behind the scenes</li> </ul>"},{"location":"labs/hbase/lab2/index.html#overview","title":"Overview","text":"<p>We will run some queries in order to check efficiency </p>"},{"location":"labs/hbase/lab2/index.html#tasks","title":"Tasks","text":"<p>Important</p> <p>The tasks below mention user-0XX, where <code>XX</code> matches your assigned Scavenger Hunt UserID.</p> <ul> <li> <p>Connect to HUE (https://cod-1eevbst1maraf-gateway.cldr-gre.yovj-8g7d.cloudera.site/cod-1eevbst1maraf/cdp-proxy/hue/hue)</p> </li> <li> <p>Check data is loaded to the table</p> </li> </ul> <pre><code>SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt; LIMIT 100;\n</code></pre> <ul> <li>Check eficciency of select by SENSOR_ID</li> </ul> <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt; WHERE SENSOR_ID='SENSOR_007' \n</code></pre> <ul> <li>Check eficciency of select by STATUS_MSG</li> </ul> <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt; WHERE STATUS_MSG='LOW_VOLTAGE' LIMIT 100;\n</code></pre>"},{"location":"labs/hbase/lab2/index.html#checklist","title":"Checklist","text":"<ul> <li> Can you explain why we have a full table scan?</li> </ul> <p> We have now concluded Lab 2 </p>"},{"location":"labs/hbase/lab3/index.html","title":"Lab 3 - HBase CRUD Operations","text":""},{"location":"labs/hbase/lab3/index.html#optimizong-hbase-tables","title":"Optimizong HBase tables","text":"<p>Goals</p> <ul> <li> Create the new HBase table that will get better performance</li> <li> Loading the data from the old table </li> <li> Creating index over the data</li> <li> Understanding range scans</li> </ul>"},{"location":"labs/hbase/lab3/index.html#overview","title":"Overview","text":"<p>We will create a new table that fits our queries in order to improve performance</p>"},{"location":"labs/hbase/lab3/index.html#tasks","title":"Tasks","text":"<p>Important</p> <p>The tasks below mention user-0XX, where <code>XX</code> matches your assigned Scavenger Hunt UserID. All Python code will run on CAI</p> <ul> <li>create the Phoenix connection</li> </ul> <pre><code>import phoenixdb\nimport phoenixdb.cursor\nfrom datetime import datetime\nimport random\nimport time\nimport urllib3\n\n# 1. SETUP &amp; AUTHENTICATION\n# Suppress SSL warnings for lab environments\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Replace with your specific Cloudera Workload Credentials\nopts = {\n    'authentication': 'BASIC',\n    'user': '&lt;User&gt;', \n    'password': '&lt;WL Pasword&gt;',\n    'verify': False \n}\n\n\nurl = \"&lt;phoenix python url&gt;\"\n\nprint(\"Connecting to Cloudera Phoenix...\")\nconn = phoenixdb.connect(url, autocommit=True, **opts)\ncursor = conn.cursor()\n</code></pre> <ul> <li> <p>create the new table</p> </li> <li> <p>We added here slat buckets , compression and changed primary key order to get better performance over SENSOR_ID scanning </p> </li> </ul> <pre><code>print(\"\\n[Step 3] Creating Optimized Table...\")\n    # Optimization: Lead with SENSOR_ID, add Salt Buckets, and Compression.\n\ncursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; (\n            SENSOR_ID VARCHAR NOT NULL,\n            EVENT_TIME TIMESTAMP NOT NULL,\n            READING DOUBLE,\n            STATUS_MSG VARCHAR,\n            CONSTRAINT pk PRIMARY KEY (SENSOR_ID, EVENT_TIME)\n        ) SALT_BUCKETS = 8, COMPRESSION = 'GZ'\n    \"\"\")\n</code></pre> <ul> <li>Now lets build our index for scanning over STATUS_MSG and populate the data</li> </ul> <pre><code>cursor.execute(\"CREATE INDEX IF NOT EXISTS IDX_STATUS ON SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; (STATUS_MSG)\")\nprint(\"[Step 4] Migrating data internally (UPSERT SELECT)...\")\n# This moves data server-side without pulling 50k rows to Python.\nstart_mig = time.time()\ncursor.execute(\"\"\"\n        UPSERT INTO SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; (SENSOR_ID, EVENT_TIME, READING, STATUS_MSG)\n        SELECT SENSOR_ID, EVENT_TIME, READING, STATUS_MSG FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt;\n    \"\"\")\nprint(f\"  Migration finished in {time.time() - start_mig:.2f}s\")\n</code></pre>"},{"location":"labs/hbase/lab3/index.html#checklist","title":"Checklist","text":"<ul> <li> Can we see data in the new table?</li> </ul> <p> We have now concluded Lab 3 </p>"},{"location":"labs/hbase/lab4/index.html","title":"Lab 4 - HBase Advanced Features","text":""},{"location":"labs/hbase/lab4/index.html#query-the-optimized-tables-data","title":"Query the optimized tables data","text":"<p>Goals</p> <ul> <li> Login to HUE </li> <li> Query the Tables data and understand what happens behind the scenes</li> </ul>"},{"location":"labs/hbase/lab4/index.html#overview","title":"Overview","text":"<p>We will run some queries in order to check efficiency </p>"},{"location":"labs/hbase/lab4/index.html#tasks","title":"Tasks","text":"<p>Important</p> <p>The tasks below mention user-0XX, where <code>XX</code> matches your assigned Scavenger Hunt UserID.</p> <ul> <li> <p>Connect to HUE (URL)</p> </li> <li> <p>Check data is loaded to the table</p> </li> </ul> <pre><code>SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; LIMIT 100;\n</code></pre> <ul> <li>Check efficiency of select by SENSOR_ID</li> </ul> <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; WHERE SENSOR_ID='SENSOR_007' LIMIT 100;\n</code></pre> <ul> <li>Check efficiency of select by STATUS_MSG</li> </ul> <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; WHERE STATUS_MSG='LOW_VOLTAGE' LIMIT 100;\n</code></pre> <p>So if we have created an index why are we still perfroming a full table scan??</p> <p>Index table only maps the column indexed and the primary key only How can we solve this problem?</p> <ul> <li>Query also primary key</li> </ul> <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; WHERE STATUS_MSG='LOW_VOLTAGE' AND SENSOR_ID='SENSOR_007' LIMIT 100;\n</code></pre> <ul> <li>Now lets try the same on the old table to see the diference <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt; WHERE STATUS_MSG='LOW_VOLTAGE' AND SENSOR_ID='SENSOR_007' LIMIT 100;\n</code></pre></li> </ul> <p>So what else can we do if we also want to have another column queried with the STATUS_MSG?</p> <ul> <li>Add a hint for the table to use the index <pre><code>SELECT \n/*+ INDEX(SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; IDX_STATUS) */ \n*\nFROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; \nWHERE STATUS_MSG = 'LOW_VOLTAGE'\nLIMIT 100;\n</code></pre></li> </ul> <p>We can also create a covered index that will contain another column's data</p> <ul> <li>Lets create another table with a covered index using also the column READING and populate it</li> </ul> <pre><code>print(\"\\n[Step 3] Creating Optimized Table...\")\n# Optimization: Lead with SENSOR_ID, add Salt Buckets, and Compression.\ncursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS SENSORS_&lt;userxxx&gt;.SENSOR_DATA_COVERED_&lt;userxxx&gt; (\n            SENSOR_ID VARCHAR NOT NULL,\n            EVENT_TIME TIMESTAMP NOT NULL,\n            READING DOUBLE,\n            STATUS_MSG VARCHAR,\n            CONSTRAINT pk PRIMARY KEY (SENSOR_ID, EVENT_TIME)\n        ) SALT_BUCKETS = 8, COMPRESSION = 'GZ'\n\"\"\")\ncursor.execute(\"CREATE INDEX IDX_STATUS_COVERED ON SENSORS_&lt;userxxx&gt;.SENSOR_DATA_COVERED_&lt;userxxx&gt; (STATUS_MSG) INCLUDE (READING)\")\nprint(\" Migrating data internally (UPSERT SELECT)...\")\n# This moves data server-side without pulling 50k rows to Python.\nstart_mig = time.time()\ncursor.execute(\"\"\"\n        UPSERT INTO SENSORS_&lt;userxxx&gt;.SENSOR_DATA_COVERED_&lt;userxxx&gt; (SENSOR_ID, EVENT_TIME, READING, STATUS_MSG)\n        SELECT SENSOR_ID, EVENT_TIME, READING, STATUS_MSG FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_NAIVE_&lt;userxxx&gt;\n\"\"\")\nprint(f\"  Migration finished in {time.time() - start_mig:.2f}s\")\n</code></pre> <ul> <li>And now we don't need to use the hint (be aware that if we had more columns they won't be covered)</li> </ul> <pre><code>Explain SELECT *\nFROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_COVERED_&lt;userxxx&gt;\nWHERE STATUS_MSG = 'LOW_VOLTAGE'\nLIMIT 100;\n</code></pre> <p>Skip scan </p> <p>The Skip Scan leverages SEEK_NEXT_USING_HINT of HBase Filter. It stores information about what set of keys/ranges of keys are being searched for in each column. It then takes a key (passed to it during filter evaluation), and figures out if it's in one of the combinations or ranges or not. If not, it figures out which next highest key to jump.</p> <pre><code>EXPLAIN SELECT * FROM SENSORS_&lt;userxxx&gt;.SENSOR_DATA_ADVANCED_&lt;userxxx&gt; WHERE SENSOR_ID IN ('SENSOR_001', 'SENSOR_007');\n</code></pre> <p>Local indexes </p> <p>Local indexes are better for write-heavy tables because the index data lives on the same RegionServer as the actual data, reducing network traffic.</p> <pre><code>CREATE LOCAL INDEX IDX_LOCAL_STATUS ON SENSORS_&lt;userxxx&gt;.SENSOR_DATA_COVERED_&lt;userxxx&gt; (STATUS_MSG);\n</code></pre> <p>If data alraedy exists in the table we can create an async index and than populate the data using a mapreduce</p> <p>for example : </p> <p>${HBASE_HOME}/bin/hbase org.apache.phoenix.mapreduce.index.IndexTool   --schema MY_SCHEMA --data-table MY_TABLE --index-table ASYNC_IDX   --output-path ASYNC_IDX_HFILES</p> <p> We have now concluded Lab 4 </p>"},{"location":"labs/kafka/index.html","title":"Index","text":""},{"location":"labs/kafka/mud/index.html","title":"run me","text":""},{"location":"labs/kafka/mud/index.html#run-me","title":"run me","text":""},{"location":"labs/kafka/mud/index.html#comannds","title":"comannds","text":"<p>create stack</p> <pre><code>kubectl annotate storageclass gp3 storageclass.kubernetes.io/is-default-class=true \n\noc create namespace kafka-mud\n\nkubectl create secret docker-registry cloudera-registry-secret   --namespace kafka-mud   --docker-server container.repository.cloudera.com   --docker-username XX   --docker-password XX\n</code></pre> <p>create kafka</p> <pre><code>oc apply --filename kafka-mud.yml,kafkanodepool.yml --namespace kafka-mud\n\n\n\nk9s -n kafka-mud\n\ns for shell\n\n/opt/kafka/bin/kafka-topics.sh --bootstrap-server mud-kafka-kafka-bootstrap:9092 --create --topic my-topic\n/opt/kafka/bin/kafka-topics.sh --bootstrap-server mud-kafka-kafka-bootstrap:9092 --describe\n\n/opt/kafka/bin/kafka-console-producer.sh --bootstrap-server mud-kafka-kafka-bootstrap:9092 --topic my-topic\n/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server mud-kafka-kafka-bootstrap:9092 --topic my-topic --from-beginning\n</code></pre>   ./kafka-topics.sh --bootstrap-server gre-smm-dh-corebroker2.cldr-gre.yovj-8g7d.cloudera.site:9093,gre-smm-dh-corebroker1.cldr-gre.yovj-8g7d.cloudera.site:9093,gre-smm-dh-corebroker0.cldr-gre.yovj-8g7d.cloudera.site:9093 --describe  ./kafka-topics.sh --bootstrap-server localhost:9093 --describe  cat &gt;/tmp/kafka-ssl.properties &lt;Click to expand fast commands for test datahub cli"},{"location":"labs/lab0/index.html","title":"Opening - Access the Environment","text":""},{"location":"labs/lab0/index.html#access-cloudera-environment","title":"Access Cloudera Environment","text":"<p>Goals</p> <ul> <li> Welcome</li> <li> Use Case Overview</li> <li> Connect to Cloudera on cloud control plane</li> </ul>"},{"location":"labs/lab0/index.html#the-scenario","title":"The Scenario","text":"<p>Pager shrieked. 3:17 AM. Seriously? Who the hell is calling me at this ungodly hour? Groaning, I swatted at the bedside table, finally silencing the infernal device. 'Infamous PCI,' my phone screen mocked me. Of course, it had to be the middle of the night.  The air in the office always had that faint aroma of impending doom, a mix of stale coffee and the lingering fear of a massive data breach. I stumbled towards my desk, the keyboard lights reflecting in my bleary eyes. The dashboard indicates an increase in suspicious transactions...</p> <p>Man, 3:25 AM, an unholy hour when only pizza delivery drivers and rogue algorithms were supposed to be awake, I'll have to follow our newly reviewed procedure \"Operation TransactionBusters\" and check some parameters before calling my manager that we got robbed...</p>"},{"location":"labs/lab0/index.html#establishing-access-to-the-cloudera-on-cloud-environment","title":"Establishing Access to the Cloudera on cloud Environment","text":"<p>Before you can start looking for frauds and other cool stuff, confirm that you can access the system.</p> <p>We will be using today Cloudera on Cloud, Connect to the Cloudera Control Plane via our login portal.</p> <p>Use the credentials that you selected for your group in this spreadsheet.</p> <p></p>  - Go to the this link.      https://raw.githubusercontent.com/22itay/hol-nifi-hbase/refs/heads/main/content/labs/lab0/fastlinks.ps1  - edit `&lt;&lt;&gt;&gt;` in line 2 &amp; 3 with the values from your browser. - run in your PC Powershell cli Click to expand fast links guide"},{"location":"labs/lab0/index.html#checklist","title":"Checklist","text":"<ul> <li> Are you able to connect to the Cloudera Control Plane?</li> <li> Do you see in Environments  list <code>cldr-gre-cdp-env</code> ?</li> <li> Do you see in Datahubs at least one <code>datahub</code> ?</li> <li> optional set fast links</li> </ul> <p> We have now concluded <code>Welcome Lab</code> </p>"},{"location":"labs/nifi/index.html","title":"Cloudera NiFi Edge2AI Hands on Lab","text":""},{"location":"labs/nifi/index.html#cloudera-nifi-edge2ai-hands-on-lab","title":"Cloudera NiFi Edge2AI Hands on Lab","text":"<p>The goal of this workshop is to give you some hands-on experience with the Cloudera's Data in Motion stack. You will be able to walk through an end-to-end (or \"edge to AI\") use case using Cloudera Edge Management, Cloudera Data Flow, Cloudera Streams Processing, Cloudera Streaming Analytics and Cloudera AI Inference products.</p> <p>What to expect</p> <p>This scavenger hunt Hands on Lab challenges you to understand a real-time fraud detection system built on Cloudera's leading data stack.</p> <p>You'll investigate suspicious transactions by tracing the data flow from edge to real-time SQL analytics. This builds upon the successful Edge2Ai Fraud transaction analytics, updated to promote certain features.</p>"},{"location":"labs/nifi/index.html#the-stack","title":"The Stack","text":"<ul> <li>MiNiFi: Generates synthetic transaction events and routes them to Kafka.</li> <li>Edge Flow Manager (EFM): Manages the entire fleet of deployed MiNiFi agents.</li> <li>DataFlow/NiFi: Copy transaction events from edgeTopic to fraudulent Topic for further analysis</li> <li>Schema Registry: Stores and manages the schema for transaction events.</li> <li>Kafka: Receives and distributes transaction events to consumers.</li> </ul> <p>Tip</p> <p>Use the workshop instructions to follow the flow, but also use the time to explore the product and learn new things and tricks. We expect you to be more confident with the product and lead workshops with customers with what you learn here today.</p> <ul> <li> <p> Lab 0. Access Cloudera Environment</p> <p>Goals</p> <ul> <li> Use Case Overview</li> <li> Connect to Cloudera on cloud control plane</li> </ul> <p> Go to Lab 0 Guide</p> </li> <li> <p> Lab 1. Getting Data from the Edge</p> <p>Goals</p> <ul> <li> Explore Edge Flow Manager interface</li> <li> Monitor and manage edge MiNiFi agents</li> </ul> <p> Go to Lab 1 Guide</p> </li> <li> <p> Lab 2. Managing Schemas with Schema Registry</p> <p>Goals</p> <ul> <li> Understand the role of Schema Registry in data pipeline architecture</li> <li> Locate, examine and Verify the FinTransactions schema in Schema Registry</li> </ul> <p> Go to Lab 2 Guide</p> </li> <li> <p> Lab 3. Receive Edge Data in Data Flow</p> <p>Goals</p> <ul> <li> Verify data flowing from edge MiNiFi agents to the Cloudera Data Flow deployment</li> <li> Examing utlization and statistics of the deployed flow</li> <li> Become familar with the components that make up the Data Flow</li> </ul> <p> Go to Lab 3 Guide</p> </li> <li> <p> Lab 4. Moving Data with NiFi DataFlow</p> <p>Goals</p> <ul> <li> Configure and deploy a flow from the Flow Catalog</li> <li> Monitor flow performance using KPIs to ensure data isn't backing up in the pipeline</li> <li> Verify end-to-end data flow from edge data topic to fraud analysis topic in Streams Messaging Manager</li> </ul> <p> Go to Lab 4 Guide</p> </li> <li> <p> Lab 5. Using Kafka to Move Data Across Applications</p> <p>Goals</p> <ul> <li> Monitor specific Kafka topics using Streams Messaging Manager</li> <li> Verify data is flowing from the NiFi deployment to downstream Kafka topics</li> </ul> <p> Go to Lab 5 Guide</p> </li> <li> <p> Lab 6. AI with NiFi DataFlow</p> <p>Goals</p> <ul> <li> Deploy NiFi DataFlow to integrate with Cloudera AI Inference service</li> <li> Process transaction data through an AI model</li> <li> Configure Kafka topics for AI-enhanced data pipeline</li> </ul> <p> Go to Lab 6 Guide</p> </li> </ul>"},{"location":"labs/nifi/lab1/index.html","title":"Lab 1 - MiNiFi and Edge Flow Manager","text":""},{"location":"labs/nifi/lab1/index.html#getting-data-from-the-edge","title":"Getting Data from the Edge","text":"<p>Goals</p> <ul> <li> Explore Edge Flow Manager interface</li> <li> Monitor and manage edge MiNiFi agents</li> </ul>"},{"location":"labs/nifi/lab1/index.html#overview","title":"Overview","text":"<p>The data that you are going to analyze is collected across several point of sales (PoS) devices your company has spread across the country. It contains account and transaction details as well as geographical coordinates of where the transactions took place.</p> <p>You have several MiNiFi agents deployed across your country-wide network to collect this data where it's generated, do some processing locally and forward it to your main data center. Cloudera Edge Management is used to deploy and manage the operation of all these agents.</p> <p>You need to check if things are running smoothly and your data is flowing correctly before you start analyzing the transactions.</p>"},{"location":"labs/nifi/lab1/index.html#tasks","title":"Tasks","text":"<p>Important</p> <p>The tasks below mention agent-0XX, where <code>XX</code> matches your assigned Scavenger Hunt UserID.</p> <ul> <li> <p>Navigate to Data Hub Clusters and click on the <code>gre-efm-dh</code> Data Hub cluster.</p> </li> <li> <p>Click on the Edge Flow Manager UI link to navigate into Edge Flow Manager</p> </li> <li> <p>Navigate to the Flow Design page. and find your Agent, in the end of the row, in the options select <code>Edit Flow</code>. Or double click on the line.</p> </li> <li> <p>Now find a way to <code>publish</code> your flow.</p> </li> </ul> <p>Tip</p> <p>you do not need to edit the flow we will be coming back to play with it shortly</p> <ul> <li> <p>To view agent health, navigate to the Monitor page. A green check indicates that the agent is reachable and heartbeating. Clicking on a specific agent you will be able to see some agent metrics</p> </li> <li> <p>When checking the agents' queues we would expect that their size are zero, which means that there's no back-pressure and all the data is being processed in a timely fashion.</p> </li> <li> <p>To view the flow that the agent is running, go back to the Flow Design page and go to your specific agent.</p> </li> <li> <p>If an agent is not running the latest version of a flow, publish the flow via the Flow Options option in the top right corner and clicking Publish.</p> </li> <li> <p>Once the flow is published wait a minute or so and then review the checklist items.</p> </li> </ul> <p>Tip</p> <p>If you can't find the Data Hub Clusters icon, click on the \"tic-tac-toe\" button at the top-left corner to expand the Cloudera Management Console sidebar menu</p>"},{"location":"labs/nifi/lab1/index.html#checklist","title":"Checklist","text":"<ul> <li> Are all our agents healthy?</li> <li> What's the size of the agents' queues (see tips)?</li> <li> What's the status for the queue metrics for <code>agent-0XX</code>?</li> <li> How many flow versions there are for <code>agent-0XX</code>?</li> <li> Is <code>agent-0XX</code> running with the correct parameters? Are you using your agent.id?? (if not fix that !!)</li> <li> Is <code>agent-0XX</code> running the latest flow??</li> </ul> <p> We have now concluded Lab 2 </p>"},{"location":"labs/nifi/lab2/index.html","title":"Lab 2 - Schema Registry","text":""},{"location":"labs/nifi/lab2/index.html#managing-schemas-with-schema-registry","title":"Managing Schemas with Schema Registry","text":"<p>Goals</p> <ul> <li> Understand the role of Schema Registry in data pipeline architecture</li> <li> Locate, examine and Verify the FinTransactions schema in Schema Registry</li> </ul>"},{"location":"labs/nifi/lab2/index.html#overview","title":"Overview","text":"<p>Well, it looks like we're receiving data from the edge as expected. So let's move on to the next step in our pipeline.</p> <p>In the next sections, we'll continue to process the data and will have to access specific fields contained in the transaction payload. For that, applications need to know what's the structure of that data and they use \"Schemas\" for that. Schemas are simply a description of the data structure that all applications can use as a contract to exchange information.</p> <p>We have a schema already defined for the transaction data we are using and it has been registered centrally in our Schema Registry so that all applications can access and use the same schema.</p> <p>Before we start processing the data in the next step, let's check if everything is ok with the schema information.</p>"},{"location":"labs/nifi/lab2/index.html#tasks","title":"Tasks","text":"<ul> <li> <p>Navigate to Data Hub Clusters and click on the <code>gre-smm-dh</code> Data Hub cluster.</p> </li> <li> <p>Click on the Schema Registry link to navigate into Schema Registry</p> </li> <li> <p>The name of the schema we'll be using is FinTransactions.</p> </li> <li> <p>Try creating a schema to see the different options you can specify.</p> </li> <li> <p>Navigate to Data Hub Clusters and click on the <code>gre-smm-dh</code> Data Hub cluster.</p> </li> <li> <p>Click on the Streams Messaging Manager link to navigate into SMM.</p> </li> <li> <p>Find your topic name <code>edge-data-0XX</code> and click on it to see its partitions.</p> </li> <li> <p>Click on any of the partitions and then \"Explore\" to browse its data. </p> </li> </ul> <p>Tip</p> <p>If you can't find the Data Hub Clusters icon, click on the \"tic-tac-toe\" button at the top-left corner to expand the Cloudera Management Console sidebar menu</p>"},{"location":"labs/nifi/lab2/index.html#checklist","title":"Checklist","text":""},{"location":"labs/nifi/lab2/index.html#schema-registry","title":"Schema Registry","text":"<ul> <li> What's the compatibility type set for the <code>FinTransactions</code> schema?</li> <li> What are the different options for compatibility when creating a new schema?</li> <li> What are the data types of the <code>lat</code> and <code>long</code> columns?</li> </ul>"},{"location":"labs/nifi/lab2/index.html#streams-messaging-manager","title":"Streams Messaging Manager","text":"<ul> <li> How many partitions does the <code>edge-data-0XX</code> topic have?</li> <li> What is the configured replication factor? Minumum in-sync replicas?</li> </ul> <p> We have now concluded Lab 3 </p>"},{"location":"labs/nifi/lab3/index.html","title":"Lab 3 - Receive Edge data in Data Flow","text":""},{"location":"labs/nifi/lab3/index.html#receiving-edge-data-with-data-flow","title":"Receiving edge data with Data Flow","text":"<p>Goals</p> <ul> <li> Verify data flowing from edge MiNiFi agents to the Cloudera Data Flow deployment</li> <li> Examing utlization and statistics of the deployed flow</li> <li> Become familar with the components that make up the Data Flow</li> </ul>"},{"location":"labs/nifi/lab3/index.html#overview","title":"Overview","text":"<p>The flow that is being executed by the MiNiFi agents forward the data to a Data Flow, running in your Cloudera Public Cloud environment. Now that you have verified that your agents are running fine and sending data, check the Data Flow deployment to make sure that the data is being received successfully.</p>"},{"location":"labs/nifi/lab3/index.html#tasks","title":"Tasks","text":"<p>Warning</p> <p>Do not make ANY changes to the \"Receive Edge Data\" flow. This flow is shared with everybody in the workshop</p> <ul> <li> <p>Navigate to Cloudera Data Flow</p> </li> <li> <p>To check the flows that are currently executing, click on the Deployment menu</p> </li> <li> <p>Click on the Receive Edge Data deployment to see its details</p> </li> <li> <p>In the deployment details tab, click on Actions &gt; View in NiFi.</p> </li> </ul> <p>Tip</p> <p>If you can't find the Cloudera Data Flow icon icon, click on the \"tic-tac-toe\" button at the top-left corner to expand the Cloudera Management Console sidebar menu</p>"},{"location":"labs/nifi/lab3/index.html#checklist","title":"Checklist","text":"<ul> <li> Is the production flow Receive Edge Data deployed and running healthily?</li> <li> Does this send data out? If so, what's the average throughput for data sent?</li> <li> What's the average CPU and memory utilization for this flow?</li> <li> How many processors are there in this flow and what do they do? (see tips)</li> </ul> <p> We have now concluded Lab 4 </p>"},{"location":"labs/nifi/lab4/index.html","title":"Lab 4 - Moving Data with NiFi DataFlow","text":""},{"location":"labs/nifi/lab4/index.html#moving-data-with-nifi-dataflow","title":"Moving Data with NiFi DataFlow","text":"<p>Goals</p> <ul> <li> Configure and deploy a flow from the Flow Catalog</li> <li> Monitor flow performance using KPIs to ensure data isn't backing up in the pipeline</li> <li> Verify end-to-end data flow from edge data topic to fraud analysis topic in Streams Messaging Manager</li> </ul>"},{"location":"labs/nifi/lab4/index.html#overview","title":"Overview","text":"<p>So lets move on to the next step in our pipeline: moving data into Kafka topics that will feed our fraud analytics application. We have already designed a flow for taking care of this but it's not running yet. It is stored securely in the central flow catalog. Let's deploy it and get it running.</p>"},{"location":"labs/nifi/lab4/index.html#tasks","title":"Tasks","text":"<p>Important</p> <p>The tasks below mention <code>&lt;your workshop username&gt;</code>. This should be your assigned unique username used to access the Cloudera on cloud tenant.</p> <ol> <li>Click on the Catalog menu in Data Flow.</li> <li>Select the flow called Kafka-To-Kafka TxN for Fraud analysis With ParamGroup</li> <li>Click on Deploy in the details pane<ol> <li>In the New Deployment dialog:<ol> <li>Target Workspace: <code>cldr-gre-cdp-env</code></li> </ol> </li> <li>In the Overview page:<ol> <li>Deployment Name: <code>&lt;your workshop username&gt; fraud flow</code></li> <li>Target Project: Select the option <code>Unassigned</code></li> </ol> </li> <li>In the Nifi Configuration page:<ol> <li>Nifi Runtime: <code>Nifi 2.x</code></li> </ol> </li> <li>In the Parameters page:<ol> <li>CDP Workload User: <code>&lt;your workshop username&gt;</code></li> <li>CDP Workload User Password: <code>&lt;your workshop password&gt;</code></li> <li>Filter Rule: <code>SELECT * FROM FLOWFILE</code></li> <li>Kafka Destination Topic: <code>fraudulent-data-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Kafka Producer ID: <code>producer-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Kafka Source Topic: <code>edge-data-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Kafka consumer Group ID: <code>consumer-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Schema Name: <code>FinTransactions</code></li> <li>Leave remaining parameters in the pla-parameter Group  as is</li> </ol> </li> <li>In the Sizing &amp; Scaling page:<ol> <li>NiFi Node Sizing: <code>extraSmall</code></li> <li>Auto Scaling: <code>disabled</code></li> <li>Storage Selection: <code>Standard</code></li> </ol> </li> <li>In the Key Performance Indicators page add a new KPI:<ol> <li>KPI Scope: <code>Entire Flow</code></li> <li>Metric to Track: <code>Flow Files Queued</code> (This KPI will show you whether data is backing up in the flow)</li> <li>Trigger alert when metric is greater than: <code>100</code></li> <li>Alert will be triggered when metric is outside the boundary(s) for: <code>5 minutes</code></li> </ol> </li> <li>Click the Deploy button to start the flow execution.</li> </ol> </li> <li>Check the deployed flow details</li> <li>Click on Action &gt; Manage Deployment to see more information</li> </ol>"},{"location":"labs/nifi/lab4/index.html#tips","title":"Tips","text":"<ul> <li> <p>The NiFi deployment is being created and should complete in 2-3 minutes. If it takes longer or errors out, please let the instructors know.</p> </li> <li> <p>Once the deployment has completed, it will take a few seconds for the Data Received/Sent gauge to be updated. You can now explore the deployment to complete the remaining tasks of this section. Explore the NiFi UI and the Deployment Manager.</p> </li> </ul>"},{"location":"labs/nifi/lab4/index.html#checklist","title":"Checklist","text":"<ul> <li> Deploy the Kafka-To-Kafka TxN for Fraud analysis With ParamGroup flow from the Catalog</li> <li> What is the number of flow files queued for your running deployment?</li> <li> What is the actual value for the Data Input Format and Data Output Format parameters for the running deployment?</li> <li> What's the topic name that the new flow deployment is writing to?</li> <li> Can you see new data coming into this topic in Streams Messaging Manager?</li> </ul> <p> We have now concluded Lab 5 </p>"},{"location":"labs/nifi/lab5/index.html","title":"Lab 5 - Using Kafka to Move Data Across Applications","text":""},{"location":"labs/nifi/lab5/index.html#using-kafka-to-move-data-across-applications","title":"Using Kafka to Move Data Across Applications","text":"<p>Goals</p> <ul> <li> Monitor specific Kafka topics using Streams Messaging Manager</li> <li> Verify data is flowing from the NiFi deployment to downstream Kafka topics</li> </ul>"},{"location":"labs/nifi/lab5/index.html#overview","title":"Overview","text":"<p>In our line of business we need that data be available for our analytical applications without delays. Kafka is an excellent messaging service that allows our applications to exchange a large amount of data extremely fast, with very low latency.</p> <p>We need to ensure that Kafka is running ok at all times and a monitoring tool like Streams Messaging Manager is extremely helpful to us.</p> <p>Before delving into the transaction analytics, take a few moments to ensure Kafka is running smoothly.</p>"},{"location":"labs/nifi/lab5/index.html#tasks","title":"Tasks","text":"<ul> <li> <p>Navigate to Streams Messaging Manager, which is located in gre-smm-dh Data Hub.</p> </li> <li> <p>In Streams Messaging Manager, explore the menus on the left-hand menu bar. If the meaning of the icons are not intuitive, you can expand the bar by clicking the chevron (<code>&gt;&gt;</code>) at the bottom.</p> </li> </ul> <p></p> <ul> <li>Click the Topics drop down at the top of the Overview screen to search for topic names</li> </ul> <p></p> <ul> <li> <p>Click the magnifying glass at any topic item to open the Data Explorer and browse topic messages for that topic</p> </li> <li> <p>If you can't find the Data Hub Clusters icon, click on the \"tic-tac-toe\" menu at the top-left corner</p> </li> </ul>"},{"location":"labs/nifi/lab5/index.html#checklist","title":"Checklist","text":"<ul> <li> How much data has the topic <code>edge-data-&lt;Scavenger_Hunt_UserID&gt;</code> received in the past hour? And in the last 24 hours?</li> <li> What is the number of flow files queued for your running deployment?</li> <li> What is the actual value for the Data Input Format and Data Output Format parameters for the running deployment?</li> <li> What's the topic name that the new flow deployment is writing to?</li> <li> Can you see new data coming into this topic in Streams Messaging Manager?</li> </ul> <p> We have now concluded Lab 6 </p>"},{"location":"labs/nifi/lab6/index.html","title":"Lab 6 - AI with NiFi DataFlow","text":""},{"location":"labs/nifi/lab6/index.html#ai-with-nifi-dataflow","title":"AI with NiFi DataFlow","text":"<p>Goals</p> <ul> <li> Deploy NiFi DataFlow to intecept Fraud Transactions</li> <li> Process transaction data through an AI model</li> <li> Configure Kafka topics for AI-enhanced data pipeline</li> </ul>"},{"location":"labs/nifi/lab6/index.html#overview","title":"Overview","text":"<p>This has been great. Our Cyber Team was able to flag fraud transactions with \"xxx\" on the transaction_id.  Now we can investigate further to confirm whether they are actually fraud. Good job team!</p> <p>Our Principal Data Scientist has come up with a new AI workflow that is able to give us some insights from the transaction data itself. Let's use Data Flow to create a new topic which we can pass all these fraud transactions to our Data Science Team.</p>"},{"location":"labs/nifi/lab6/index.html#tasks","title":"Tasks","text":"<p>Important</p> <p>The tasks below mention <code>&lt;your workshop username&gt;</code>. This should be your assigned unique username used to access the Cloudera on cloud tenant.</p> <ol> <li>Click on the Catalog menu in Data Flow.</li> <li>Select the flow called Edge2AI-Fraud-Transaction</li> <li>Click on Deploy in the details pane<ol> <li>In the New Deployment dialog:<ol> <li>Target Workspace: <code>cldr-gre-cdp-env</code></li> </ol> </li> <li>In the Overview page:<ol> <li>Deployment Name: <code>&lt;your workshop username&gt; AI Fraud</code></li> <li>Target Project: Select the option <code>Unassigned</code></li> </ol> </li> <li>In the Nifi Configuration page:<ol> <li>Nifi Runtime: <code>Nifi 2.x</code></li> </ol> </li> <li>In the Parameters page:<ol> <li>Filter Rule: <code>SELECT * FROM FLOWFILE WHERE transaction_id LIKE 'xxx%'</code> </li> <li>Consumer Group ID: <code>fraud-consumer-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Source Topic: <code>edge-data-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Target Topic: <code>fraud-topic-&lt;Scavenger_Hunt_UserID&gt;</code></li> <li>Workload Username: <code>&lt;your workshop username&gt;</code></li> <li>Workload Password: <code>&lt;your workshop password&gt;</code></li> <li>Leave remaining parameters in the pla-parameter Group  as is</li> </ol> </li> <li>In the Sizing &amp; Scaling page:<ol> <li>NiFi Node Sizing: <code>extraSmall</code></li> <li>Auto Scaling: <code>disabled</code></li> <li>Storage Selection: <code>Standard</code></li> </ol> </li> <li>In the Key Performance Indicators click Next.</li> </ol> </li> <li>Click on Deploy to start the flow execution</li> <li>Check the deployed flow details</li> <li>Click on Action &gt; Manage Deployment to see more information</li> </ol>"},{"location":"labs/nifi/lab6/index.html#tips","title":"Tips","text":"<ul> <li> <p>The NiFi deployment is being created and should complete in 2-3 minutes. If it takes longer or errors out, please let the instructors know.</p> </li> <li> <p>Once the deployment has completed, it will take a few seconds for the Data Received/Sent gauge to be updated. You can now explore the deployment to complete the remaining tasks of this section. Explore the NiFi UI and the Deployment Manager.</p> </li> </ul>"},{"location":"labs/nifi/lab6/index.html#checklist","title":"Checklist","text":"<ul> <li> Deploy the Edge2AI-Fraud-Transaction flow from the Catalog</li> <li> What's the processor used on the flow to filter fraud transactions?</li> <li> Check the target topic in Streams Messaging Manager to confirm it is all fraud transactions.</li> </ul> <p> We have now concluded Lab 7 </p> <p> This concludes our Hands on Workshop </p>"}]}